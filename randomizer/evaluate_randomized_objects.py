from benchmark import question
from benchmark.problems import problem
import pickle
import argparse
import pandas as pd

parser = argparse.ArgumentParser(description='Evaluate the generated prompts')
parser.add_argument('--input', type=str, default='random_prompts.pkl', help='Input file path for the prompt pickled objects')
parser.add_argument('--output', type=str, default='evaluation_results.csv', help='Output file path for the evaluation results')
parser.add_argument('--model_response', type=str, default='model_responses.json', help='Output file path for the model responses')
parser.add_argument('--original_prompts', type=str, default='random_prompts.json', help='The original prompts that were generated by the randomizer')
args = parser.parse_args()

def evaluate_prompts(args) :
    '''
    Evaluate the prompts
    args : arguments from the command line
    '''
    model_responses = list(pd.read_json(args.model_response)['Response'])
    prompts_constraints_json = pd.read_json(args.original_prompts)
    original_prompts = list(prompts_constraints_json['Prompt'])
    original_constraints = list(prompts_constraints_json['Constraint Type'])
    with open(args.input, 'rb') as f:
        prompt_objects = pickle.load(f)
    results = []
    for prompt in range(len(prompt_objects)) :
        try:
            if isinstance(prompt_objects[prompt], question.Question) :
                results.append(prompt_objects[prompt].evaluate_response(model_responses[prompt]))
            elif isinstance(prompt_objects[prompt], problem.Problem) :
                results.append((prompt_objects[prompt].validate(model_responses[prompt]),None,None,None))
            else :
                results.append((prompt_objects[prompt].validate(model_responses[prompt], original_constraints[prompt]),None,None,None))
        except Exception as e:
            print(e)
            results.append((False, e, e, e))
    df = pd.DataFrame(results, columns = ['Correctness', 'Violated Constraints', 'Partial Correctness', 'Satisfied Constraints'])
    df.to_csv(args.output, index = False)
    print("Evaluation results saved to", args.output)

evaluate_prompts(args)